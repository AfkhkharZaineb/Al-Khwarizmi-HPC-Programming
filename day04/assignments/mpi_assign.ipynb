{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Hello World\n",
    "\n",
    "1. Write an MPI program displaying the number of processes used for the execution and the rank of each process.\n",
    "2. Test the programs obtained with different numbers of threads for the parallel program.\n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "Hello from the rank 2 process\n",
    "Hello from the rank 0 process\n",
    "Hello from the rank 3 process\n",
    "Hello from the rank 1 process\n",
    "Parallel execution of hello_world with 4 process\n",
    "```\n",
    "*Note that the output order maybe different*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_mpi.py\n"
     ]
    }
   ],
   "source": [
    "%%file hello_mpi.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "n = COMM.Get_size()\n",
    "\n",
    "RANK = COMM.Get_rank()\n",
    "print(\"hello from the rank {RANK} thread\".format(RANK = RANK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpirun -n 3 python hello_mpi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results : \n",
    "\n",
    "hello from the rank 0 thread\n",
    "\n",
    "hello from the rank 1 thread\n",
    "\n",
    "hello from the rank 2 thread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2 Sharing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common need is for one process to get data from the user, either by reading from the terminal or command line arguments, and then to distribute this information to all other processors.\n",
    "\n",
    "Write a program that reads an integer value from the terminal and distributes the value to all of the MPI processes. Each process should print out its rank and the value it received. Values should be read until a negative integer is given as input.\n",
    "\n",
    "You may want to use these MPI routines in your solution: Get_rank Bcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sharing.py\n"
     ]
    }
   ],
   "source": [
    "%%file sharing.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank( )\n",
    "if RANK == 0 :\n",
    "    sendbuf = int(input())\n",
    "else :\n",
    "    sendbuf = None\n",
    "\n",
    "recvbuf = COMM.bcast(sendbuf , root=0 )\n",
    "print( \"the rank : {RANK} got the value : {recvbuf}\".format(RANK=RANK,recvbuf=recvbuf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter command for compile and run the program\n",
    "#mpirun -n 6 python hati.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results : \n",
    "\n",
    "2\n",
    "\n",
    "the rank : 2 got the value : 2\n",
    "\n",
    "the rank : 3 got the value : 2\n",
    "\n",
    "the rank : 4 got the value : 2\n",
    "\n",
    "the rank : 5 got the value : 2\n",
    "\n",
    "the rank : 0 got the value : 2\n",
    "\n",
    "the rank : 1 got the value : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Sending in a ring (broadcast by ring)\n",
    "\n",
    "Write a program that takes data from process zero and sends it to all of the other processes by sending it in a ring. That is, process i should receive the data and send it to process i+1, until the last process is reached.\n",
    "Assume that the data consists of a single integer. Process zero reads the data from the user.\n",
    "![](../data/ring.gif)\n",
    "\n",
    "You may want to use these MPI routines in your solution:\n",
    "`Send` `Recv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ring.py\n"
     ]
    }
   ],
   "source": [
    "%%file ring.py\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "#print(RANK)\n",
    "sendbuf =  1000\n",
    "tag = 9\n",
    "if RANK == 0 :\n",
    "    COMM.send(sendbuf,dest= RANK + 1 , tag=tag )\n",
    "    print( \"Process \"+str(RANK)+\" got \"+str(sendbuf)+\"\\n\")\n",
    "elif RANK < SIZE - 1 :\n",
    "    COMM.send(sendbuf,dest= RANK + 1 , tag=tag )\n",
    "    print( \"Process \"+str(RANK)+\" got \"+str(sendbuf)+\" from the process : \"+str(RANK-1)+\"\\n\")\n",
    "else :\n",
    "    COMM.recv( source= RANK-1,tag=tag)\n",
    "    print( \"Process \"+str(RANK)+\" got \"+str(sendbuf)+\" from the process : \"+str(RANK-1)+\"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpirun -n 6 python ring.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results : \n",
    "\n",
    "Process 3 got 1000 from the process : 2\n",
    "\n",
    "Process 4 got 1000 from the process : 3\n",
    "\n",
    "Process 5 got 1000 from the process : 4\n",
    "\n",
    "Process 0 got 1000\n",
    "\n",
    "Process 1 got 1000 from the process : 0\n",
    "\n",
    "Process 2 got 1000 from the process : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 Matrix vector product\n",
    "\n",
    "1. Use the `MatrixVectorMult.py` file to implement the MPI version of matrix vector multiplication.\n",
    "2. Process 0 compares the result with the `dot` product.\n",
    "3. Plot the scalability of your implementation. \n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "CPU time of parallel multiplication using 2 processes is  174.923446\n",
    "The error comparing to the dot product is : 1.4210854715202004e-14\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MatrixVectorMult_V0.py\n"
     ]
    }
   ],
   "source": [
    " %%file MatrixVectorMult_V0.py\n",
    " # write your program here\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy.random import rand, seed\n",
    "\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbOfproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "seed(42)\n",
    "\n",
    "def matrixVectorMult(A, b, x):\n",
    "    \n",
    "    row, col = A.shape\n",
    "    for i in range(row):\n",
    "        a = A[i]\n",
    "        for j in range(col):\n",
    "            x[i] += a[j] * b[j]\n",
    "\n",
    "    return 0\n",
    "\n",
    "########################initialize matrix A and vector b ######################\n",
    "#matrix sizes\n",
    "SIZE = 1000\n",
    "#Local_size = \n",
    "\n",
    "# counts = block of each proc\n",
    "#counts = \n",
    "\n",
    "if RANK == 0:\n",
    "    A = lil_matrix((SIZE, SIZE))\n",
    "    A[0, :100] = rand(100)\n",
    "    A[1, 100:200] = A[0, :100]\n",
    "    A.setdiag(rand(SIZE))\n",
    "    A = A.toarray()\n",
    "    b = rand(SIZE)\n",
    "else :\n",
    "    A = None\n",
    "    b = None\n",
    "\n",
    "start = MPI.Wtime()\n",
    "matrixVectorMult(LocalMatrix, b, LocalX)\n",
    "stop = MPI.Wtime()\n",
    "if RANK == 0:\n",
    "    print(\"CPU time of parallel multiplication is \", (stop - start)*1000)\n",
    "if RANK == 0 :\n",
    "    X_ = A.dot(b)\n",
    "    print(\"The result of A*b using dot is :\", np.max(X_ - X))\n",
    "    # print(\"The result of A*b using parallel version is :\", X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MatrixVectorMult_V0.py\n"
     ]
    }
   ],
   "source": [
    " %%file MatrixVectorMult_V0.py\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy.random import rand, seed\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "''' This program compute parallel csc matrix vector multiplication using mpi '''\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return np.array([a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)])\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "nbOfproc = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "#print(nbOfproc)\n",
    "#print(RANK)\n",
    "seed(42)\n",
    "\n",
    "def matrixVectorMult(A, b, x):\n",
    "    \n",
    "    row, col = A.shape\n",
    "    for i in range(row):\n",
    "        a = A[i]\n",
    "        for j in range(col):\n",
    "            x[i] += a[j] * b[j]\n",
    "\n",
    "    return 0\n",
    "\n",
    "########################initialize matrix A and vector b ######################\n",
    "#matrix sizes\n",
    "SIZE = 1000\n",
    "Local_size = 500\n",
    "\n",
    "# counts = block of each proc\n",
    "counts = 4 \n",
    "\n",
    "if RANK == 0:\n",
    "    A = lil_matrix((SIZE, SIZE))\n",
    "    A[0, :100] = rand(100)\n",
    "    A[1, 100:200] = A[0, :100]\n",
    "    A.setdiag(rand(SIZE))\n",
    "    A = A.toarray(order='C')\n",
    "    b = rand(SIZE)\n",
    "    X_ = A.dot(b)\n",
    "    A = split(A, nbOfproc)\n",
    "else :\n",
    "    A = None\n",
    "    b = None\n",
    "\n",
    "\n",
    "#########Send b to all procs and scatter A (each proc has its own local matrix#####\n",
    "recvbuf = COMM.bcast(b , root=0 )\n",
    "# Scatter the matrix A\n",
    "LocalMatrix = COMM.scatter(A,root=0)\n",
    "COMM.Barrier()\n",
    "#print(\"I'm the rank : \"+str(RANK)+\" I got \"+str(LocalMatrix))\n",
    "#####################Compute A*b locally#######################################\n",
    "LocalX = np.ones((Local_size,1))\n",
    "\n",
    "start = MPI.Wtime()\n",
    "#print(RANK)\n",
    "matrixVectorMult(LocalMatrix, recvbuf, LocalX)\n",
    "stop = MPI.Wtime()\n",
    "if RANK == 0:\n",
    "    print(\"CPU time of parallel multiplication is \", (stop - start)*1000)\n",
    "##################Gather te results ###########################################\n",
    "# sendcouns = local size of result\n",
    "sendcounts = LocalX.shape\n",
    "if RANK == 0: \n",
    "    X = LocalX\n",
    "else :\n",
    "     X = LocalX\n",
    "\n",
    "# Gather the result into X\n",
    "X = COMM.gather(X,root=0)\n",
    "if RANK == 0 :\n",
    "    print(\"The result of A*b using dot is :\", X_ )\n",
    "    print(\"The result of A*b using parallel version is :\", X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpirun -n 3 python MatrixVectorMult_V0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for 3 threads is : \n",
    "\n",
    "CPU time of parallel multiplication is  1056.589517\n",
    "\n",
    "\n",
    "The result of A*b using dot is : [2.63781326e+01 2.13071154e+01 2.77823379e-01 4.71827620e-01\n",
    " 9.02944984e-01 4.33507343e-02 1.62610958e-01 5.72887944e-01\n",
    " 1.59248241e-01 1.18468930e-02 2.36388833e-01 3.61845979e-02\n",
    " 2.08082711e-01 4.33939974e-01 3.75570382e-01 5.05522269e-01\n",
    " 7.35254166e-02 1.63707925e-01 2.37063708e-01 6.98528680e-02\n",
    " 7.17610888e-01 8.56350993e-01 2.74159578e-01 8.90888020e-02\n",
    " ...\n",
    "\n",
    "The result of A*b using parallel version is : [array([[27.37813265],\n",
    "       [22.30711543],\n",
    "       [ 1.27782338],\n",
    "       [ 1.47182762],\n",
    "       [ 1.90294498],\n",
    "       [ 1.04335073],\n",
    "       [ 1.16261096],\n",
    "       [ 1.57288794],..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 Calculation of π (Monte Carlo)\n",
    "\n",
    "1. Use the `PiMonteCarlo.py` file to implement the calculation of PI using Monte Carlo.\n",
    "2. Process 0 prints the result.\n",
    "3. Plot the scalability of your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PiMonteCarlo_V0.py\n"
     ]
    }
   ],
   "source": [
    "%%file PiMonteCarlo_V0.py\n",
    " # write your program here\n",
    "import random \n",
    "import timeit\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "COMM = MPI.COMM_WORLD\n",
    "size = COMM.Get_size()\n",
    "rank= COMM.Get_rank()\n",
    "\n",
    "INTERVAL= 1000\n",
    "\n",
    "random.seed(42)  \n",
    "if rank == 0:\n",
    "    total = np.zeros(1)\n",
    "else:\n",
    "    total = None\n",
    "def compute_points():  \n",
    "    \n",
    "    circle_points= np.zeros(1)\n",
    "\n",
    "    # Total Random numbers generated= possible x \n",
    "    # values* possible y values \n",
    "    num_per_rank = INTERVAL**2 // size # the floor division // rounds the result down to the nearest whole number.\n",
    "    lower_bound = 1 + rank * num_per_rank\n",
    "    upper_bound = 1 + (rank + 1) * num_per_rank\n",
    "    print(\"This is processor \", rank, \"and I am executing the loop from\", lower_bound,\" to \", upper_bound - 1, flush=True)\n",
    "    COMM.Barrier()\n",
    "    for i in range(lower_bound, upper_bound):\n",
    "        \n",
    "        # Randomly generated x and y values from a \n",
    "        # uniform distribution \n",
    "        # Rannge of x and y values is -1 to 1 \n",
    "                \n",
    "        rand_x= random.uniform(-1, 1) \n",
    "        rand_y= random.uniform(-1, 1) \n",
    "      \n",
    "        # Distance between (x, y) from the origin \n",
    "        origin_dist= rand_x**2 + rand_y**2\n",
    "      \n",
    "        # Checking if (x, y) lies inside the circle \n",
    "        if origin_dist<= 1: \n",
    "            circle_points[0]+= 1\n",
    "      \n",
    "        # Estimating value of pi, \n",
    "        # pi= 4*(no. of points generated inside the  \n",
    "        # circle)/ (no. of points generated inside the square) \n",
    "    \n",
    "    COMM.Barrier() \n",
    "    # collect the partial results and add to the total sum\n",
    "    COMM.Reduce(circle_points, total, op=MPI.SUM, root=0)\n",
    "    \n",
    "    return total\n",
    "\n",
    "start = timeit.default_timer()\n",
    "circle_points = compute_points()\n",
    "end = timeit.default_timer()\n",
    "\n",
    "\n",
    "pi = 4* circle_points/ INTERVAL**2 \n",
    "print(\"Circle points number :\",circle_points)\n",
    "print(\"Final Estimation of Pi=\", pi, \"cpu time :\",end-start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter command for compile and run the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result with 6 process :\n",
    "\n",
    "This is processor  3 and I am executing the loop from 499999  to  666664\n",
    "\n",
    "This is processor  4 and I am executing the loop from 666665  to  833330\n",
    "\n",
    "This is processor  0 and I am executing the loop from 1  to  166666\n",
    "\n",
    "This is processor  2 and I am executing the loop from 333333  to  499998\n",
    "\n",
    "This is processor  1 and I am executing the loop from 166667  to  333332\n",
    "\n",
    "This is processor  5 and I am executing the loop from 833331  to  999996\n",
    "\n",
    "Circle points number : [785028.]\n",
    "Final Estimation of Pi= [3.140112] cpu time : 0.2647160682827234"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
